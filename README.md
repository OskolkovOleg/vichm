Ниже — ответы **по прикреплённой теории** + **короткая строка “Шпора”** к каждому пункту.

---

## 1) Классификация погрешностей

**Ответ:** В теории выделяют погрешности по источнику: **моделирования**, **округления**, **метода**, **вычислений** (и отдельно — погрешности средств измерения).
Также часто делят по устранимости: **устранимые** (можно уменьшать улучшением метода/точности/модели) и **неустранимые** (ограничения разрядной сетки, неизбежные допущения и т.п.).

**Шпора:** *Δ = (модель + округление + метод + вычисления [+ измерения]).*

---

## 2) Абсолютная и относительная погрешность величины и функции

**Ответ:**

* Абсолютная погрешность: (\Delta x = |x-\tilde x|).
* Относительная: (\delta x = \dfrac{\Delta x}{|x|}) (или (\Delta x/|\tilde x|) в практике).
  Для функции аналогично: (\Delta f = |f-\tilde f|), (\delta f = \Delta f/|f|).

**Шпора:** *Абс: |точн−прибл|, отн: Абс/|значение|.*

---

## 3) Погрешность функции. Вывод формул

**Ответ:** Вывод идёт через разложение Тейлора / дифференциал. Для (y=f(x)):
[
\Delta y \approx |f'(x)|,\Delta x
]
(главная часть погрешности). Для нескольких переменных:
[
\Delta f \approx \sum_{i}\left|\frac{\partial f}{\partial x_i}\right|\Delta x_i
]
как оценка по полному дифференциалу.

**Шпора:** *Δf ≈ |f'|Δx; многомерн: Σ|∂f/∂xi|Δxi.*

---

## 4) Источники погрешностей

**Ответ:**

* **Моделирования**: упрощения, нереалистичные предположения, пропущенные факторы. 
* **Округления**: конечная разрядность/значащие цифры. 
* **Метода**: аппроксимации и ограничения применимости численного метода. 
* **Вычислений**: накопление ошибок округления при операциях. 

**Шпора:** *Модель / округление / метод / накопление вычислений.*

---

## 5) Основные этапы анализа погрешностей

**Ответ (по смыслу теории):**

1. задать входные данные и их погрешности;
2. получить формулу/оценку распространения погрешности (через производные/дифференциал);
3. оценить итоговую погрешность результата и выделить доминирующие источники;
4. при необходимости — уменьшить шаг, повысить точность, улучшить модель/метод. (Связано с выделением источников и оценками через главную часть.)

**Шпора:** *Задал Δвход → протащил через формулу → получил Δвыход → уменьшаю источники.*

---

## 6) Количество верных знаков

**Ответ:** Количество верных знаков находят через проверку неравенства
[
\Delta x \le 0.5\cdot 10^{,l},
]
ищут минимальное (l), затем решают
[
k = m + l + 1,
]
где (m) — старший разряд числа. 

**Шпора:** *Ищи l из Δx ≤ 0.5·10^l, потом k = m+l+1.*

---

## 7) Погрешности моделирования

**Ответ:** Возникают из-за различий между реальной системой и моделью: **упрощения**, **предположения**, **пропуск факторов**. 

**Шпора:** *Модель ≠ реальность: упрощения/предположения/пропуски.*

---

## 8) Задача интерполяции

**Ответ:** По набору табличных значений ((x_i, y_i)) построить функцию (обычно полином), которая **точно проходит через узлы**, и по ней вычислять значения внутри интервала. 

**Шпора:** *Интерполяция: найти (P), чтобы (P(x_i)=y_i).*

---

## 9) Интерполяционные узлы, интерполяционный полином

**Ответ:** **Узлы** — точки (x_0,\dots,x_n) (обычно попарно различные), в которых известны значения (y_i=f(x_i)). **Интерполяционный полином** (P_n(x)) степени (\le n), удовлетворяющий (P_n(x_i)=y_i). 

**Шпора:** *Узлы (x_i), полином (P_n): (P_n(x_i)=y_i).*

---

## 10) Теорема о единственности интерполяционного полинома

**Ответ:** Если существуют два полинома степени (\le n), совпадающие во всех (n+1) различных узлах, то их разность имеет (n+1) корней ⇒ разность тождественно ноль ⇒ полином единственен. 

**Шпора:** *Два (P) совпали в (n+1) узлах ⇒ (P_1=P_2).*

---

## 11) Полином Лагранжа: формула, плюсы/минусы, свойства базисных

**Ответ:**
Форма:
[
P_n(x)=\sum_{i=0}^n y_i,\ell_i(x),\quad
\ell_i(x)=\prod_{j\ne i}\frac{x-x_j}{x_i-x_j}.
]
Свойство базисных: (\ell_i(x_j)=\delta_{ij}). 
**Достоинства:** явная формула, не нужны разности/таблицы.
**Недостатки:** при добавлении узла пересчитывается весь полином; при больших (n) возможна неустойчивость/осцилляции. 

**Шпора:** *Лагранж: (P=\sum y_i\ell_i), (\ell_i(x_j)=\delta_{ij}).*

---

## 12) Полином Ньютона: формула, плюсы/минусы

**Ответ:** Полином Ньютона строится по **разделённым разностям**:
[
P_n(x)=a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\cdots
]
где (a_k) — разделённые разности. 
**Плюсы:** удобно добавлять новый узел (добавляется новый член).
**Минусы:** нужно считать таблицу разностей/разделённых разностей. 

**Шпора:** *Ньютон: наращиваем по узлам; коэффициенты — разделённые разности.*

---

## 13) Полином Ньютона 2-го рода

**Ответ:** Это форма Ньютона, “идущая от конца” (удобна при равномерной сетке и конечных разностях): полином записывают через конечные разности, но опорной точкой берут правый край сетки. 

**Шпора:** *Ньютон-2: то же, но от правого конца (через конечные разности).*

---

## 14) Оператор конечной разности

**Ответ:** Для равномерной сетки шаг (h):
[
\Delta y_i = y_{i+1}-y_i,\quad \Delta^2 y_i=\Delta(\Delta y_i),\ \ldots
]
Это дискретный аналог производных.

**Шпора:** *Δy = y_{i+1}-y_i; Δ²y = Δ(Δy).*

---

## 15) Сплайн-интерполяция. Условия кубических сплайнов

**Ответ:** Кубический сплайн — кусочно-кубическая функция на каждом ([x_i,x_{i+1}]):
[
S_i(x)=a_i+b_i(x-x_i)+c_i(x-x_i)^2+d_i(x-x_i)^3.
]
Условия:

1. (S(x_i)=y_i) (интерполяция),
2. непрерывность (S, S', S'') во внутренних узлах,
3. граничные условия (например, “натуральный”: (S''(x_0)=S''(x_n)=0)). 

**Шпора:** *Сплайн: кубики по кускам + (S,S',S'') непрерывны + граничные условия.*

---

## 16) Задача аппроксимации

**Ответ:** По данным ((x_i,y_i)) найти функцию (\varphi(x)) “простого вида”, которая **не обязана** проходить через все точки, но **хорошо приближает** их по выбранному критерию ошибки. 

**Шпора:** *Аппроксимация: “почти” по точкам, не обязательно точно.*

---

## 17) Виды аппроксимации

**Ответ:** В зависимости от класса функций: линейная/нелинейная (по параметрам), полиномиальная, сплайновая и др.; плюс выбор критерия (максимальная ошибка, среднеквадратичная и т.п.). 

**Шпора:** *Выбираем класс (\varphi(x)) + критерий ошибки.*

---

## 18) Метод наименьших квадратов

**Ответ:** Подбирают параметры (\varphi(x,\theta)), минимизируя сумму квадратов невязок:
[
S(\theta)=\sum_{i=1}^{n}(y_i-\varphi(x_i,\theta))^2 \to \min.
]
Для линейной аппроксимации прямой (y\approx a+bx) берут (\partial S/\partial a=0), (\partial S/\partial b=0) и получают систему нормальных уравнений для (a,b). 

**Шпора:** *МНК: минимизируем (\sum (y_i-\varphi_i)^2) ⇒ нормальные уравнения.*

---

## 19) Численное решение уравнений. Постановка задачи

**Ответ:** Дано уравнение (f(x)=0). Нужно найти значения (x), при которых (f(x)=0). Точное решение в общем виде часто невозможно ⇒ ищут **приближённое** с заданной точностью. Обычно 2 этапа: **отделение корней** и **уточнение**. 

**Шпора:** *(f(x)=0): отделить корень → уточнить до (\varepsilon).*

---

## 20) Корень уравнения. Решение уравнения

**Ответ:** Корень — значение (x), при котором (f(x)=0). Решить уравнение — найти все такие (x) (или нужный корень на заданном интервале). 

**Шпора:** *Корень: (f(x)=0). Решение: найти такие (x).*

---

## 21) Различие алгебраических и трансцендентных уравнений

**Ответ (классика):**

* **Алгебраическое**: полиномиальное (a_nx^n+\cdots+a_0=0).
* **Трансцендентное**: содержит (\sin,\cos,\exp,\ln,) и т.п., не сводится к многочлену (например (\sin x = x/2)).

**Шпора:** *Алгебр.=многочлен; трансценд.=триг/exp/log и т.п.*

---

## 22) Геометрический метод отделения корней

**Ответ:** Графически (или через исследование знаков) находят интервалы, где корень возможен: смотрят смену знака (f(a)\cdot f(b)<0), а чтобы гарантировать “один корень” — учитывают монотонность (через знак (f'(x)) и интервалы монотонности).

**Шпора:** *Отделение: найти [a,b], где (f(a)f(b)<0) и 1 корень (монотонность).*

---

## 23) Метод половинного деления: алгоритм, условия, скорость

**Ответ:**
**Условия:** (f) непрерывна на ([a,b]), (f(a)\cdot f(b)<0).
**Алгоритм:** (c=(a+b)/2), выбирают половину отрезка, где есть смена знака, повторяют.
**Скорость:** линейная; оценка длины интервала/ошибки:
[
|b_n-a_n|\le \frac{b-a}{2^n}
]
(в теории дано как оценка (3.2) на примере). 

**Шпора:** *Биссекция: делим пополам, берём кусок со сменой знака; ошибка ~ ((b-a)/2^n).*

---

## 24) Метод простой итерации: алгоритм, условия, скорость

**Ответ:** Представляют (f(x)=0) как (x=\varphi(x)). Итерации:
[
x_{k+1}=\varphi(x_k).
]
**Условия сходимости:** (\varphi([a,b])\subset[a,b]) и (|\varphi'(x)|\le q<1).
**Скорость:** геометрическая:
[
|x_n-x^*|\le q^n|x_0-x^*|.
]


**Шпора:** *Итерации: (x_{k+1}=\varphi(x_k)), нужно (|\varphi'|\le q<1), скорость (q^n).*

---

## 25) Метод хорд: алгоритм, условия, скорость

**Ответ:** Берут хорду через (A=(a,f(a))), (B=(b,f(b))), точка пересечения с (Ox) даёт новое приближение:
[
c=a-\frac{b-a}{f(b)-f(a)},f(a).
]
Дальше сужают интервал по знакам (f) и повторяют до (|x_n-x_{n-1}|\le\varepsilon). 
**Условия:** нужен отрезок с одним корнем и непрерывность (f). 
**Скорость (классика):** обычно быстрее биссекции; метод секущих имеет сверхлинейную сходимость порядка (\approx 1.618) (если “как секущие” без постоянного удержания концов).

**Шпора:** *Хорда: (c=a-\frac{b-a}{f(b)-f(a)}f(a)), обычно быстрее биссекции.*

---

## 26) Метод касательных: алгоритм, условия, скорость

**Ответ:** Касательная в точке (z):
[
c=z-\frac{f(z)}{f'(z)}.
]
Важно выбирать (z) так, чтобы знаки (f(z)) и (f''(z)) совпадали (тогда уточнение “идёт внутрь” интервала).
Останов: (|x_n-x_{n-1}|\le\varepsilon). 
**Скорость (классика):** квадратичная в окрестности корня при хорошей стартовой точке.

**Шпора:** *Касательные: (x_{k+1}=x_k-\frac{f}{f'}), старт: (f\cdot f''>0).*

---

## 27) Метод Ньютона: алгоритм, условия, скорость

**Ответ:** Для уравнения (f(x)=0) метод Ньютона совпадает с методом касательных:
[
x_{k+1}=x_k-\frac{f(x_k)}{f'(x_k)}.
]
Нужны (f') (и обычно (f'')) на интервале и удачное начальное приближение; критерий остановки — по (\varepsilon).
**Скорость (классика):** квадратичная при выполнении условий.

**Шпора:** *Ньютон: (x_{k+1}=x_k-\frac{f}{f'}), быстро сходится при хорошем (x_0).*

---

## 28) Численное интегрирование: постановка задачи, зачем нужно, примеры

**Ответ:** Нужно вычислить
[
I=\int_a^b f(x),dx.
]
Но не все интегралы берутся по Ньютону–Лейбницу (например, (\int_0^1 e^{-x^2}dx) не выражается в элементарных функциях), первообразная может быть слишком сложной, а (f(x)) может быть задана таблицей. Поэтому используют **квадратурные формулы**:
[
\int_a^b f(x),dx \approx \sum_{i=0}^{n}A_i f(x_i),
]
где (x_i) — узлы, (A_i) — веса. 

**Шпора:** *Интеграл часто “не берётся”/таблица ⇒ (\int \approx \sum A_i f(x_i)).*

---

Если хочешь — я могу **сжать всё это в 1 страницу “шпоры”** (только формулы + условия + порядок/скорость), без пояснений.


Ниже ответы **(по твоей теории)** + в конце каждого пункта **коротко “на шпору”**.

---

## 29. Методы прямоугольников. Вывод формул

**Идея вывода:** разбиваем ([a,b]) на (n) частей: (x_i=a+ih,; h=\frac{b-a}{n}). Интеграл (\int_a^b f(x),dx) заменяем суммой площадей прямоугольников на каждом ([x_i,x_{i+1}]): (S_i \approx h\cdot f(\xi_i)). От выбора (\xi_i) получаем разные формулы.

* **Левые прямоугольники:** (\xi_i=x_i)
  [
  I_{\text{л}} = h\sum_{i=0}^{n-1} f(x_i)
  ]
* **Правые прямоугольники:** (\xi_i=x_{i+1})
  [
  I_{\text{пр}} = h\sum_{i=1}^{n} f(x_i)
  ]
* **Средние прямоугольники:** (\xi_i=\frac{x_i+x_{i+1}}2)
  [
  I_{\text{ср}} = h\sum_{i=0}^{n-1} f!\left(\frac{x_i+x_{i+1}}2\right)
  ]
  Оценка погрешности для “средних прямоугольников” через (f'') в теории дана как (формула типа (\propto h^2)).

**Шпора:**
Прямоуг.: (I\approx h\sum f(\xi_i)); лев: (x_i), прав: (x_{i+1}), средн: (\frac{x_i+x_{i+1}}2). 

---

## 30. Методы трапеций, парабол. Вывод формул

### Метод трапеций

**Вывод:** на каждом ([x_i,x_{i+1}]) заменяем график (f(x)) **отрезком** (линейной интерполяцией) ⇒ площадь ≈ площадь трапеции.
Составная формула:
[
I_{\text{тр}}=\frac{h}{2}\Bigl(f(x_0)+2\sum_{i=1}^{n-1}f(x_i)+f(x_n)\Bigr)
]
Для уточнения/оценки погрешности в теории приведено **правило Рунге** (сравнение шага (h) и (h/2)). 

### Метод парабол (Симпсона)

**Вывод:** на паре отрезков ([x_{2k},x_{2k+2}]) аппроксимируем (f(x)) **параболой** (квадратичной интерполяцией по трём точкам) и интегрируем её. Составная формула (при чётном (n)):
[
I_{\text{Симп}}=\frac{h}{3}\Bigl(f(x_0)+4\sum_{i\ \text{нечёт}} f(x_i)+2\sum_{i\ \text{чёт},, i\neq n} f(x_i)+f(x_n)\Bigr)
]
Для Симпсона тоже дано правило Рунге (коэффициент (1/15)).

**Шпора:**
Трап.: (\frac h2(f_0+2\sum f_i+f_n)).
Симпсон: (\frac h3(f_0+4\sum f_{\text{неч}}+2\sum f_{\text{чёт}}+f_n)).
Рунге: трап/прямоуг (\frac13(I^{h/2}-I^h)), Симпсон (\frac1{15}(I^{h/2}-I^h)). 

---

## 31. Квадратурные формулы Гаусса. Весовые коэффициенты и узлы

**Формула на ([-1,1]):**
[
\int_{-1}^{1} f(x),dx \approx \sum_{i=1}^{n}\omega_i f(\xi_i),
]
где (\xi_i) — **узлы**, (\omega_i) — **веса**.

**Как выбираются узлы:** (\xi_i) берут корнями полинома Лежандра (P_n(x)). В теории дано определение (P_n) и пример корней для (n=3). 

**Как находятся веса:** веса (\omega_i) определяют из системы моментов (в теории приведена система для (n=3)) и получены (\omega_1=\omega_3=\frac59,; \omega_2=\frac89). 

(Идея: подбираем (\omega_i), чтобы формула была точной на полиномах максимально высокой степени.)

**Шпора:**
Гаусс: (\int_{-1}^1 f \approx \sum \omega_i f(\xi_i)), (\xi_i) — корни (P_n). Для (n=3): (\xi={-\sqrt{3/5},0,\sqrt{3/5}}), (\omega={5/9,8/9,5/9}). 

---

## 32. Постановка задачи решения СЛАУ

Дано (A\in\mathbb{R}^{n\times n}), (b\in\mathbb{R}^n). Найти (x\in\mathbb{R}^n) из:
[
Ax=b.
]


**Шпора:** (Ax=b) (найти (x)). 

---

## 33. Классификация численных методов решения СЛАУ

* **Прямые методы** (за конечное число шагов в точной арифметике): Гаусс, QR/ортогонализация и т.п.
* **Итерационные методы** (строят последовательность (x^{(k)}\to x)): простые итерации, Зейдель и др.

**Шпора:** прямые (Гаусс, QR), итерационные (Якоби/простые итерации, Зейдель).

---

## 34. Метод Гаусса

**Суть:** элементарными преобразованиями строк приводим матрицу к **верхнетреугольному виду** (прямой ход), затем делаем **обратную подстановку** (обратный ход). В теории показано выполнение прямого и обратного ходов на расширенной матрице. 

**Шпора:** Гаусс = прямой ход (обнуление ниже диагонали) + обратный ход (подстановка). 

---

## 35. Метод Гаусса с выбором главного элемента

**Зачем:** деление на “плохие” (малые) диагональные элементы усиливает ошибки и может дать деление на ноль.
**Идея:** на шаге (k) выбирают **главный элемент** (обычно максимальный по модулю в столбце/подматрице) и делают перестановки строк/столбцов, затем продолжают исключение. В теории показан пример выбора главного элемента и оговорка про изменение порядка переменных при перестановке столбцов.

**Шпора:** pivoting: выбираем max (|a_{ik}|), меняем строки (и при полном — ещё столбцы), дальше обычный Гаусс. 

---

## 36. Число обусловленности матрицы. Проблемы и примеры плохо обусловленных матриц

**Определение (классика):**
[
\operatorname{cond}(A)=|A|\cdot|A^{-1}|.
]
**Смысл:** показывает, во сколько раз могут **усилиться относительные ошибки** (в данных/округлении) в решении. Большое (\operatorname{cond}(A)) ⇒ система **плохо обусловлена**, вычислительная ошибка резко растёт.

**Проблемы:** сильная чувствительность к округлениям и к малым возмущениям (A) или (b); “красивый” ответ на компьютере может стать неверным. В теории прямо отмечено, что величина ошибки связана с обусловленностью, а “исправить” её без изменения матрицы нельзя. 

**Пример плохо обусловленной:** матрица Гильберта (H_{ij}=\frac1{i+j-1}) — классический пример (cond быстро растёт с (n)).

**Шпора:** cond(=|A||A^{-1}|); большая cond ⇒ ошибки сильно усиливаются (пример: Гильберт). 

---

## 37. Метод ортогонализации матриц Грамма–Шмидта

**Идея:** разложить (A=QR), где (Q) — ортогональная, (R) — верхнетреугольная. (Q) строится ортогонализацией столбцов (A) по Грамму–Шмидту. Тогда
[
Ax=b ;\Rightarrow; QRx=b ;\Rightarrow; Rx=Q^Tb
]
и решаем треугольную систему обратным ходом.

**Шпора:** QR через Грамма–Шмидта; решаем (Rx=Q^Tb). 

---

## 38. Метод простых итераций. Условие сходимости

**Приведение:** СЛАУ приводят к виду
[
x = Bx + c
]
и строят итерации
[
x^{(k+1)} = Bx^{(k)} + c.
]
**Условие сходимости (достаточное в теории):** (\max_k |\lambda_k(B)|<1) (эквивалентно (\rho(B)<1)). 

**Шпора:** (x^{k+1}=Bx^k+c), сходится если (\rho(B)<1) (или max(|\lambda(B)|<1)). 

---

## 39. Метод Зейделя. Условия сходимости

Разложение (A=C+D+H) (нижняя/диагональ/верхняя) и итерация:
[
(D+H)x^{(k+1)}=-Cx^{(k)}+b
]
(в компонентной форме: новое (x_i^{(k+1)}) использует уже обновлённые значения). 

**Условие сходимости:** как и у итерационных — (\rho(B_{GS})<1) (для матрицы итераций метода). Достаточные практические условия часто формулируют через диагональное преобладание/положительную определённость (если это оговаривали на лекциях). 

**Шпора:** Зейдель: “обновляем по мере вычисления”; сходимость при (\rho(B_{GS})<1) (часто достаточно диагонального преобладания). 

---

## 40. Постановка задачи решения ОДУ

Найти функцию (y(x)), удовлетворяющую ОДУ вида
[
y'=f(x,y)
]
на интервале ([a,b]) (или более общий вид), часто при заданных начальных/граничных условиях.

**Шпора:** решить (y'=f(x,y)) на ([a,b]) с условиями.

---

## 41. Задача Коши

Это ОДУ + **начальное условие** в точке:
[
y'=f(x,y),\qquad y(x_0)=y_0.
]

**Шпора:** (y'=f(x,y),; y(x_0)=y_0).

---

## 42. Метод Эйлера. Порядок точности

Сетка (x_{k+1}=x_k+h). Формула:
[
y_{k+1}=y_k+h,f(x_k,y_k).
]
Порядок: **1-й** (глобальная погрешность (O(h)), локальная (O(h^2))). 

**Шпора:** Эйлер: (y_{k+1}=y_k+h f(x_k,y_k)), порядок 1. 

---

## 43. Улучшенный метод Эйлера. Порядок точности

(Хойна / “предиктор–корректор”)

1. предиктор: (y^*=y_k+h f(x_k,y_k))
2. корректор:
   [
   y_{k+1}=y_k+\frac{h}{2}\Big(f(x_k,y_k)+f(x_{k+1},y^*)\Big)
   ]
   Порядок: **2-й**.

**Шпора:** улучш. Эйлер: (y^*=y_k+h f_k), (y_{k+1}=y_k+\frac h2(f_k+f_{k+1}^*)), порядок 2.

---

## 44. Метод Рунге–Кутты. Порядок точности

Классический RK4:
[
\begin{aligned}
k_1&=f(x_k,y_k),\
k_2&=f(x_k+\tfrac h2,,y_k+\tfrac h2 k_1),\
k_3&=f(x_k+\tfrac h2,,y_k+\tfrac h2 k_2),\
k_4&=f(x_k+h,,y_k+h k_3),\
y_{k+1}&=y_k+\tfrac h6,(k_1+2k_2+2k_3+k_4).
\end{aligned}
]
Порядок: **4-й**.

**Шпора:** RK4: (y_{k+1}=y_k+\frac h6(k_1+2k_2+2k_3+k_4)), порядок 4.

---

## 45. Метод последовательных приближений

Это метод Пикара для задачи Коши. Переход к интегральному уравнению:
[
y(x)=y_0+\int_{x_0}^{x} f(t,y(t)),dt,
]
и итерации:
[
y_{n+1}(x)=y_0+\int_{x_0}^{x} f(t,y_n(t)),dt,\quad y_0(x)\equiv y_0.
]

**Шпора:** Пикар: (y_{n+1}=y_0+\int f(t,y_n),dt).

---

## 46. Метод изоклин

**Изоклина** — кривая (f(x,y)=c), на которой наклон решений (y'=f(x,y)) одинаков (в поле направлений — одинаковый угол). По набору изоклин строят качественную картину решений.

**Шпора:** изоклины: (f(x,y)=c) (одинаковый наклон (y')).

---

## 47. Метод степенных рядов

Ищем решение в виде ряда (обычно около (x_0)):
[
y(x)=\sum_{k=0}^{\infty} a_k (x-x_0)^k,
]
подставляем в ОДУ, приравниваем коэффициенты ⇒ находим (a_k) рекуррентно из условия (y(x_0)=y_0) и ОДУ.

**Шпора:** (y=\sum a_k(x-x_0)^k), подставить в ОДУ ⇒ рекурсия для (a_k).

---

## 48. Постановка задачи оптимизации. Классификация задач оптимизации

**Постановка:** найти минимум/максимум целевой функции.

* Одномерная: (\min_{x\in[a,b]} f(x)). 
* Многомерная: (\min f(x_1,\dots,x_n)). 

**Классификация (как в теории):**

* **Без ограничений:** градиентный спуск, метод Ньютона.
* **С ограничениями:** множители Лагранжа, штрафные функции.

**Шпора:** min/max (f); без ограничений (GD, Newton), с ограничениями (Лагранж, штрафы). 

---

## 49. Метод дихотомии

Последовательно сужаем ([a,b]), сравнивая (f) в двух точках около середины. Теория: метод основан на делении интервала и выборе нового интервала по меньшему значению функции. 

**Шпора:** дихотомия: делим ([a,b]), сравниваем (f) в двух близких к середине точках ⇒ сужаем интервал. 

---

## 50. Метод золотого сечения

Улучшение дихотомии: делим интервал в пропорции золотого сечения (чтобы на следующем шаге переиспользовать одну точку). В теории описан принцип “постоянной пропорции” золотого сечения. 

**Шпора:** золотое сечение: делим ([a,b]) в “золотой” пропорции, каждый шаг — оптимально сокращаем интервал. 

---

## 51. Метод Фибоначчи

Интервальный метод: число шагов (N) фиксируют заранее, точки выбирают по отношениям чисел Фибоначчи, чтобы минимизировать худший случай длины финального интервала. 

**Шпора:** Фибо: (N) шагов задано, точки через отношения (F_k) ⇒ гарантированное сжатие. 

---

## 52. Метод Брента

Комбинирует **параболическую интерполяцию** (быстро, когда работает) и **золотое сечение** (надёжно, когда парабола “плохая”). 

**Шпора:** Брент = парабола (ускорение) + золотое сечение (страховка). 

---

## 53. Метод Ньютона (в 1D оптимизации)

Для экстремума решаем (f'(x)=0) методом Ньютона:
[
x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)}.
]
В теории указан метод Ньютона как один из методов одномерной оптимизации. 

**Шпора:** Ньютона (min): (x_{k+1}=x_k-\frac{f'}{f''}). 

---

## 54. Метод градиентного спуска

Итерационный метод без ограничений:
[
x_{k+1}=x_k-\alpha\nabla f(x_k),
]
останавливаемся при (|\nabla f(x_k)|<\varepsilon).

**Шпора:** GD: (x_{k+1}=x_k-\alpha\nabla f(x_k)), стоп при малом градиенте. 

---

## 55. Метод Ньютона (в многомерной оптимизации)

Использует градиент и Гессиан (H_k):
[
x_{k+1}=x_k-\alpha H_k^{-1}\nabla f(x_k).
]

**Шпора:** Newton nD: (x_{k+1}=x_k-\alpha H^{-1}\nabla f). 

---

## 56. Метод множителей Лагранжа

Для ограничений-равенств (g_i(x)=0) вводят
[
L(x,\lambda)=f(x)+\sum \lambda_i g_i(x),
]
и ищут стационарность по (x) + выполнение ограничений.

**Шпора:** Лагранж: (L=f+\sum \lambda_i g_i), решаем (\nabla_x L=0) и (g_i=0). 

---

## 57. Метод штрафных функций

Заменяем задачу с ограничениями на последовательность задач без ограничений:
[
\Phi(x,r)=f(x)+r,P(x),
]
где (P(x)) штрафует нарушение ограничений, (r>0) — штрафной параметр. 

**Шпора:** штрафы: (\Phi=f+rP), увеличиваем (r) ⇒ “вдавливаем” в ограничения. 

---

Если хочешь — могу одним сообщением собрать **“шпору” только формулами и условиями сходимости** (без объяснений), чтобы просто скопировать в заметки.
# vichm
